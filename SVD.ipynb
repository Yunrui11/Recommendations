{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb6b44f-aac0-4e8b-b351-879ab47f24b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 13:12:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"svdtest\").getOrCreate()\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "df = spark.read.csv(\"dataforFP.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd40fb-7837-4db6-85b9-35596727160c",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d40e6c1-4774-49ac-8be8-e8fb5d6a9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, Tokenizer, StopWordsRemover, HashingTF, IDF, PCA, Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d6ecaa3-806a-4644-a01f-4abb3929684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert LISTINGID to a unique integer index\n",
    "indexer = StringIndexer(inputCol=\"LISTINGID\", outputCol=\"listing_id_index\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# Step 2: Tokenize the TITLE column\n",
    "tokenizer = Tokenizer(inputCol=\"TITLE\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d074f594-c572-4e83-b83c-3ba1a4244f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|           CREATEDAT|              USERID|           LISTINGID|               TITLE|listing_id_index|               words|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|2024-10-03 00:03:...|1027633c-bbf0-443...|00903e30-deb2-4d3...|       Funko Arlong |          1633.0|     [funko, arlong]|\n",
      "|2024-10-03 00:55:...|b7589547-dace-456...|83fdbd98-78a9-439...|Funko SSGSS Veget...|           417.0|[funko, ssgss, ve...|\n",
      "|2024-10-03 02:04:...|70b74c2f-9648-4ec...|82c2bb33-3bcf-4dd...|    Funko Darth Maul|          1680.0|[funko, darth, maul]|\n",
      "|2024-10-03 00:42:...|9526ea4f-6281-462...|5cdae714-aa07-4f4...|Funko Tanjiro sig...|           934.0|[funko, tanjiro, ...|\n",
      "|2024-10-03 02:10:...|1c3ec79c-d28f-4ba...|5cdae714-aa07-4f4...|Funko Tanjiro sig...|           934.0|[funko, tanjiro, ...|\n",
      "|2024-10-03 00:27:...|667b013a-99f4-42c...|5cdae714-aa07-4f4...|Funko Tanjiro sig...|           934.0|[funko, tanjiro, ...|\n",
      "|2024-10-03 02:13:...|1c3ec79c-d28f-4ba...|5cdae714-aa07-4f4...|Funko Tanjiro sig...|           934.0|[funko, tanjiro, ...|\n",
      "|2024-10-03 02:04:...|3fb10768-6d54-46c...|82c2bb33-3bcf-4dd...|    Funko Darth Maul|          1680.0|[funko, darth, maul]|\n",
      "|2024-10-03 00:31:...|9526ea4f-6281-462...|5cdae714-aa07-4f4...|Funko Tanjiro sig...|           934.0|[funko, tanjiro, ...|\n",
      "|2024-10-03 02:00:...|3fb10768-6d54-46c...|d4faebbc-d826-4a1...|Funko Bane signed...|           968.0|[funko, bane, sig...|\n",
      "+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3db2751-87bf-41b3-8031-1919899544e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_data = remover.transform(words_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8142b028-59cc-4de3-8d70-7617eda6b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply TF-IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "featurized_data = hashingTF.transform(filtered_data)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "tfidf_data = idf_model.transform(featurized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bfe4d6b-8be7-4f3c-9c6a-4c0c63bc5c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CREATEDAT: timestamp (nullable = true)\n",
      " |-- USERID: string (nullable = true)\n",
      " |-- LISTINGID: string (nullable = true)\n",
      " |-- TITLE: string (nullable = true)\n",
      " |-- listing_id_index: double (nullable = false)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rawFeatures: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de7f58-9fd4-4879-a0e7-56139507361f",
   "metadata": {},
   "source": [
    "## Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2dce5a3-0275-4722-9fb4-0e9f1e97946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Apply SVD (try pca first)\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pcaFeatures\")  # Adjust k as needed\n",
    "pca_model = pca.fit(tfidf_data)\n",
    "pca_data = pca_model.transform(tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a83aab6d-073c-475b-90a3-fd9230b703e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance by PC 1: 0.0006\n",
      "Explained variance by PC 2: 0.0003\n",
      "Explained variance by PC 3: 0.0002\n",
      "Explained variance by PC 4: 0.0002\n",
      "Explained variance by PC 5: 0.0002\n",
      "Explained variance by PC 6: 0.0002\n",
      "Explained variance by PC 7: 0.0002\n",
      "Explained variance by PC 8: 0.0002\n",
      "Explained variance by PC 9: 0.0002\n",
      "Explained variance by PC 10: 0.0002\n",
      "Explained variance by PC 11: 0.0002\n",
      "Explained variance by PC 12: 0.0002\n",
      "Explained variance by PC 13: 0.0002\n",
      "Explained variance by PC 14: 0.0002\n",
      "Explained variance by PC 15: 0.0002\n",
      "Explained variance by PC 16: 0.0002\n",
      "Explained variance by PC 17: 0.0002\n",
      "Explained variance by PC 18: 0.0002\n",
      "Explained variance by PC 19: 0.0002\n",
      "Explained variance by PC 20: 0.0002\n",
      "Explained variance by PC 21: 0.0001\n",
      "Explained variance by PC 22: 0.0001\n",
      "Explained variance by PC 23: 0.0001\n",
      "Explained variance by PC 24: 0.0001\n",
      "Explained variance by PC 25: 0.0001\n",
      "Explained variance by PC 26: 0.0001\n",
      "Explained variance by PC 27: 0.0001\n",
      "Explained variance by PC 28: 0.0001\n",
      "Explained variance by PC 29: 0.0001\n",
      "Explained variance by PC 30: 0.0001\n",
      "Explained variance by PC 31: 0.0001\n",
      "Explained variance by PC 32: 0.0001\n",
      "Explained variance by PC 33: 0.0001\n",
      "Explained variance by PC 34: 0.0001\n",
      "Explained variance by PC 35: 0.0001\n",
      "Explained variance by PC 36: 0.0001\n",
      "Explained variance by PC 37: 0.0001\n",
      "Explained variance by PC 38: 0.0001\n",
      "Explained variance by PC 39: 0.0001\n",
      "Explained variance by PC 40: 0.0001\n",
      "Explained variance by PC 41: 0.0001\n",
      "Explained variance by PC 42: 0.0001\n",
      "Explained variance by PC 43: 0.0001\n",
      "Explained variance by PC 44: 0.0001\n",
      "Explained variance by PC 45: 0.0001\n",
      "Explained variance by PC 46: 0.0001\n",
      "Explained variance by PC 47: 0.0001\n",
      "Explained variance by PC 48: 0.0001\n",
      "Explained variance by PC 49: 0.0001\n",
      "Explained variance by PC 50: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate total variance of the original features\n",
    "total_variance = tfidf_data.select(\"features\").rdd \\\n",
    "    .map(lambda row: Vectors.dense(row[\"features\"].toArray()).var()) \\\n",
    "    .sum()\n",
    "\n",
    "# Calculate variance explained by each principal component\n",
    "explained_variance = []\n",
    "k = 50\n",
    "for i in range(k):\n",
    "    component_variance = pca_data.select(\"pcaFeatures\") \\\n",
    "        .rdd.map(lambda row: row[\"pcaFeatures\"][i] ** 2).mean()\n",
    "    explained_variance.append(component_variance / total_variance)\n",
    "\n",
    "# Display the explained variance ratio for each component\n",
    "for i, variance in enumerate(explained_variance):\n",
    "    print(f\"Explained variance by PC {i + 1}: {variance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834186d4-b20a-488c-9e76-1647157451bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 6: Normalize data for KNN\n",
    "normalizer = Normalizer(inputCol=\"pcaFeatures\", outputCol=\"normFeatures\")\n",
    "normalized_data = normalizer.transform(pca_data)\n",
    "\n",
    "# Step 7: Perform K-Means Clustering as an alternative to KNN\n",
    "kmeans = KMeans(featuresCol=\"normFeatures\", k=45)  # Adjust k based on data size\n",
    "model = kmeans.fit(normalized_data)\n",
    "predictions = model.transform(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf2c2c59-c1c3-410f-85f6-857c08e046ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 591:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.40533884963072436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"normFeatures\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score: {silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f582e065-f6c6-4284-bc7e-0e5275911996",
   "metadata": {},
   "source": [
    "I experimented with various numFeatures values for TF-IDF (ranging from 500 to 2000) and tested different PCA dimensions (from 50 to 150). Despite these adjustments, the explained variance per principal component never exceeded 0.0006. When running KNN, I observed that the Silhouette score only improved when the number of neighbors (K) matched the number of PCA dimensions. This suggests that the variance is distributed thinly across many dimensions, which is typical in sparse text data where each word appears in only a small subset of documents. In this context, each PCA component captures only a small fraction of the total variance, leading to low explained variance in each dimension.\n",
    "\n",
    "The nature of my data likely contributes to this challenge: the titles are short and often consist of character names from anime or movies, with limited symbolic or contextual overlap between them. As a result, text data may not be the most effective dimension for my analysis. Moving forward, I plan to focus on images to explore whether this modality yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46066d1e-89dd-4229-ac84-776ac8cef84e",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e28795-3dee-426f-a7ea-d8f2ca2e9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:32:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Images_associate\").getOrCreate()\n",
    "\n",
    "# Load your data into a DataFrame\n",
    "df = spark.read.parquet(\"image_features_with_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b6def9-bc40-45b3-b63d-b503e17f22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Step 1: Define the feature columns\n",
    "feature_columns = [str(i) for i in range(2048)]  # All columns from '0' to '2047'\n",
    "\n",
    "# Step 2: Fill null values with 0.0 in all feature columns\n",
    "df_filled = df.fillna(0.0, subset=feature_columns)\n",
    "\n",
    "# Step 3: Use VectorAssembler to combine these columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_vector\")\n",
    "df_vectorized = assembler.transform(df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd62ef21-4068-488c-b4f9-bae672c4a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:32:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     features_vector|\n",
      "+--------------------+\n",
      "|[147.0,121.0,106....|\n",
      "|[190.0,255.0,19.0...|\n",
      "|[130.0,243.0,17.0...|\n",
      "|[190.0,255.0,19.0...|\n",
      "|[147.0,121.0,106....|\n",
      "|[190.0,255.0,19.0...|\n",
      "|[190.0,255.0,19.0...|\n",
      "|[147.0,121.0,106....|\n",
      "|[190.0,255.0,19.0...|\n",
      "|[147.0,121.0,106....|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_vectorized.select(\"features_vector\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9f82fe4-aa4d-4767-a8af-a54ef82e85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectorized = df_vectorized.na.drop(subset=[\"features_vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54786169-e968-4c78-873b-c05fbef52df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply PCA on the combined vector column\n",
    "pca = PCA(k=50, inputCol=\"features_vector\", outputCol=\"pca_features\")  # Adjust 'k' as needed\n",
    "pca_model = pca.fit(df_vectorized)\n",
    "df_with_pca = pca_model.transform(df_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90cce8ae-ce6e-435d-b651-b93bd16d72b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:36:54 WARN DAGScheduler: Broadcasting large task binary with size 1507.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:36:55 WARN DAGScheduler: Broadcasting large task binary with size 1507.3 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        pca_features|\n",
      "+--------------------+\n",
      "|[-5715.4713731739...|\n",
      "|[-6035.3505252281...|\n",
      "|[-5935.7142382889...|\n",
      "|[-6035.3505252281...|\n",
      "|[-5715.4713731739...|\n",
      "|[-6035.3505252281...|\n",
      "|[-6035.3505252281...|\n",
      "|[-5715.4713731739...|\n",
      "|[-6035.3505252281...|\n",
      "|[-5715.4713731739...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_pca.select(\"pca_features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb5a147-faf5-4807-97f1-f7b5d15b17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"pca_features\", outputCol=\"norm_features\")\n",
    "normalized_data = normalizer.transform(df_with_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b50337a-987f-4a96-b02f-cf9225cd0773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:37:03 WARN DAGScheduler: Broadcasting large task binary with size 1566.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1567.6 KiB\n",
      "24/11/04 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1568.2 KiB\n",
      "24/11/04 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1568.6 KiB\n",
      "24/11/04 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1568.6 KiB\n",
      "24/11/04 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1569.0 KiB\n",
      "24/11/04 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1569.5 KiB\n",
      "24/11/04 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1568.8 KiB\n",
      "24/11/04 16:37:50 WARN DAGScheduler: Broadcasting large task binary with size 1614.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:38:10 WARN DAGScheduler: Broadcasting large task binary with size 1942.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Step 6: Perform K-Means Clustering as an alternative to KNN\n",
    "kmeans = KMeans(featuresCol=\"norm_features\", k=30)  # Adjust k based on data size\n",
    "model = kmeans.fit(normalized_data)\n",
    "predictions = model.transform(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28b3d4f0-5066-4bf1-8d05-1f226898b4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:38:11 WARN DAGScheduler: Broadcasting large task binary with size 1597.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/04 16:38:32 WARN DAGScheduler: Broadcasting large task binary with size 1625.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.038786311244633075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"norm_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score: {silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b433b7d-ba37-4d0d-a0bc-671397e9510d",
   "metadata": {},
   "source": [
    "- after running this analysis, I am not sure if this is a good idea for my project because this method is really computational intensive and not sure, but the result isn't as good as I thought. Find the best k value can take forever becasue the data is too large to be processed. The prediction evaluation isn't showing this is a really promissing method either, so I am just gonna pause here for my exploration and move on for collaborative filtering. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
